{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11913321,"sourceType":"datasetVersion","datasetId":7489704}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:05:04.664434Z","iopub.execute_input":"2025-05-23T09:05:04.664724Z","iopub.status.idle":"2025-05-23T09:05:05.023548Z","shell.execute_reply.started":"2025-05-23T09:05:04.664700Z","shell.execute_reply":"2025-05-23T09:05:05.022713Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vertex-service-sa/peppy-appliance-460613-k3-cc10f0749a25.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Semantic Search with Vertex AI Embeddings and FAISS on NFCorpus","metadata":{}},{"cell_type":"markdown","source":"## Overview\n\nIn this notebook, we will build a simple semantic search using Vertex AI's text embedding models and FAISS for semantic search.\n\nWe are using NFCCourpus Dataset.","metadata":{}},{"cell_type":"markdown","source":"## **Steps:**\n- Setup the libraries\n- Load the NFCourpus Dataset\n- Vertex AI to laoad a pre trained model for text embedding.\n- Convert documents into embeddings and build a FAISS index for fast searching.\n- Sample Test\n- Evaluation using pytrec_eval","metadata":{}},{"cell_type":"code","source":"!pip install --quiet beir faiss-cpu google-cloud-aiplatform numpy pandas pytrec_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:05:08.297283Z","iopub.execute_input":"2025-05-23T09:05:08.297681Z","iopub.status.idle":"2025-05-23T09:06:59.092114Z","shell.execute_reply.started":"2025-05-23T09:05:08.297659Z","shell.execute_reply":"2025-05-23T09:06:59.090425Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.0/288.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for pytrec_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#beir -> benchmarking Information retrieval. Benchmark framework designed to evaluate the performance of IR models across diverse tasks and dataset.\n#pytech_eval is python interface to trev_eval toolkit, which is a standard tool for evaluating IR systems.\n\nfrom beir import util\nfrom beir.datasets.data_loader import GenericDataLoader\nimport faiss\nimport vertexai\nfrom vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\nimport numpy as np\nimport pandas as pd\nimport pytrec_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:06:59.094833Z","iopub.execute_input":"2025-05-23T09:06:59.095235Z","iopub.status.idle":"2025-05-23T09:07:26.180368Z","shell.execute_reply.started":"2025-05-23T09:06:59.095196Z","shell.execute_reply":"2025-05-23T09:07:26.179436Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/beir/util.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#This function will take textual strings and convert them into their numerical embeddings using Vertex AI model.\n#We will be able to use it anywhere for embedding large corpus of data or incoming queries.\n\ndef embed_text(texts: list[str], model: TextEmbeddingModel, task: str, batch_size: int = 5) -> np.ndarray:\n    \"\"\"\n    Embeds a list of texts using a Vertex AI embedding model.\n\n    Args:\n        texts: A list of strings to embed.\n        model: The Vertex AI TextEmbeddingModel instance.\n        task: The task type for the embedding (e.g., \"RETRIEVAL_DOCUMENT\", \"RETRIEVAL_QUERY\").\n        batch_size: The number of texts to process in each batch.\n\n    Returns:\n        A NumPy array containing the embeddings.\n    \"\"\"\n    embed_mat = np.zeros((len(texts), 768))  # Assuming 768 dimensions for \"text-embedding-005\"\n    for batch_start in range(0, len(texts), batch_size):\n        size = min(len(texts) - batch_start, batch_size)\n\n        #Vertex AI SDK method doesn't take a list of raw strings, it expects TextEmbeddingInput objects.\n        #Each of these objects needs the text and a task_type.\n        inputs = [TextEmbeddingInput(texts[batch_start + i], task_type=task) for i in range(size)]\n        embeddings = model.get_embeddings(inputs)\n        for i in range(size):\n            embed_mat[batch_start + i, :] = embeddings[i].values\n    return embed_mat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:26.181368Z","iopub.execute_input":"2025-05-23T09:07:26.182025Z","iopub.status.idle":"2025-05-23T09:07:26.188686Z","shell.execute_reply.started":"2025-05-23T09:07:26.181968Z","shell.execute_reply":"2025-05-23T09:07:26.187745Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Loading NFCorpus dataset.\nIt contains corpus of medical documents, a set of queries, and relevance judgments (qrels)\n","metadata":{}},{"cell_type":"code","source":"url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip\"\ndata_path_root = \"datasets\" # Root directory for datasets\ndata_path_nfcorpus = os.path.join(data_path_root, \"nfcorpus\") # Specific path for nfcorpus","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:26.190911Z","iopub.execute_input":"2025-05-23T09:07:26.191238Z","iopub.status.idle":"2025-05-23T09:07:26.469513Z","shell.execute_reply.started":"2025-05-23T09:07:26.191214Z","shell.execute_reply":"2025-05-23T09:07:26.468490Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Create the datasets directory if it doesn't exist\nos.makedirs(data_path_root, exist_ok=True)\n\ndownloaded_data_path = util.download_and_unzip(url, data_path_root)\nprint(f\"Dataset downloaded and unzipped to: {downloaded_data_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:26.470916Z","iopub.execute_input":"2025-05-23T09:07:26.471237Z","iopub.status.idle":"2025-05-23T09:07:27.668266Z","shell.execute_reply.started":"2025-05-23T09:07:26.471206Z","shell.execute_reply":"2025-05-23T09:07:27.667413Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"datasets/nfcorpus.zip:   0%|          | 0.00/2.34M [00:00<?, ?iB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ba1864db00f4ed1a989d0e663cd0516"}},"metadata":{}},{"name":"stdout","text":"Dataset downloaded and unzipped to: datasets/nfcorpus\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Load the corpus, queries, and qrels for the \"test\" split\ncorpus, queries, qrels = GenericDataLoader(data_folder=downloaded_data_path).load(split=\"test\")\n\nprint(f\"Number of documents: {len(corpus)}\")\nprint(f\"Number of queries: {len(queries)}\")\nprint(f\"Number of query-relevance pairs: {sum(len(v) for v in qrels.values())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:27.669260Z","iopub.execute_input":"2025-05-23T09:07:27.669598Z","iopub.status.idle":"2025-05-23T09:07:27.761544Z","shell.execute_reply.started":"2025-05-23T09:07:27.669567Z","shell.execute_reply":"2025-05-23T09:07:27.760593Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3633 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb350ef4a7f4a8a818093ee56b8873a"}},"metadata":{}},{"name":"stdout","text":"Number of documents: 3633\nNumber of queries: 323\nNumber of query-relevance pairs: 12334\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#Example\ndoc_id_example, doc_example = next(iter(corpus.items()))\nquery_id_example, query_example = next(iter(queries.items()))\nprint(f\"\\nExample Document (ID: {doc_id_example}): '{doc_example['title']} {doc_example['text'][:100]}...'\")\nprint(f\"Example Query (ID: {query_id_example}): '{query_example}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:27.944145Z","iopub.execute_input":"2025-05-23T09:07:27.944404Z","iopub.status.idle":"2025-05-23T09:07:27.950671Z","shell.execute_reply.started":"2025-05-23T09:07:27.944384Z","shell.execute_reply":"2025-05-23T09:07:27.949842Z"}},"outputs":[{"name":"stdout","text":"\nExample Document (ID: MED-10): 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland Recent studies have suggested that statins, an established drug group in the prevention of cardiovas...'\nExample Query (ID: PLAIN-2): 'Do Cholesterol Statin Drugs Cause Breast Cancer?'\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install -q --upgrade vertexai google-genai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:27.951605Z","iopub.execute_input":"2025-05-23T09:07:27.951882Z","iopub.status.idle":"2025-05-23T09:07:56.127483Z","shell.execute_reply.started":"2025-05-23T09:07:27.951844Z","shell.execute_reply":"2025-05-23T09:07:56.126445Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.3/196.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ------------------ This is to be hidden ------------------ #\n#PROJECT_ID = \"Added as kaggle Secret\"\n#LOCATION = \"Added as Kaggle Secret\"   \n# -----------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:56.130684Z","iopub.execute_input":"2025-05-23T09:07:56.131033Z","iopub.status.idle":"2025-05-23T09:07:56.135487Z","shell.execute_reply.started":"2025-05-23T09:07:56.130988Z","shell.execute_reply":"2025-05-23T09:07:56.134761Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\nos.makedirs(\"Google_Service_key\", exist_ok=True)\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/kaggle/input/vertex-service-sa/peppy-appliance-460613-k3-cc10f0749a25.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:56.136605Z","iopub.execute_input":"2025-05-23T09:07:56.136907Z","iopub.status.idle":"2025-05-23T09:07:56.155981Z","shell.execute_reply.started":"2025-05-23T09:07:56.136885Z","shell.execute_reply":"2025-05-23T09:07:56.155057Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nPROJECT_ID = UserSecretsClient().get_secret(\"PROJECT_ID\")\nLOCATION = UserSecretsClient().get_secret(\"LOCATION\")\n\nvertexai.init(project=PROJECT_ID, location=LOCATION)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:56.156969Z","iopub.execute_input":"2025-05-23T09:07:56.157396Z","iopub.status.idle":"2025-05-23T09:07:56.359300Z","shell.execute_reply.started":"2025-05-23T09:07:56.157373Z","shell.execute_reply":"2025-05-23T09:07:56.358483Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"#Loading the model for embedding\nmodel_name = \"text-embedding-005\" #It has 768 dimensions\nmodel = TextEmbeddingModel.from_pretrained(model_name)\nprint(f\"Loaded Vertex AI embedding model: {model_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:56.360314Z","iopub.execute_input":"2025-05-23T09:07:56.360643Z","iopub.status.idle":"2025-05-23T09:07:57.984270Z","shell.execute_reply.started":"2025-05-23T09:07:56.360617Z","shell.execute_reply":"2025-05-23T09:07:57.983209Z"}},"outputs":[{"name":"stdout","text":"Loaded Vertex AI embedding model: text-embedding-005\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"#corpus.items()\n\n#zip and '*', the unpacking operator takes the list of tuples and \"unzips\" them into two separate tuples.\ndoc_ids, docs = zip(*[(doc_id, doc['text']) for doc_id, doc in corpus.items()])\nprint(f\"Prepared {len(docs)} documents for embedding.\")\n\n# Extract query IDs and texts\nq_ids, questions = zip(*[(q_id, q_text) for q_id, q_text in queries.items()])\nprint(f\"Prepared {len(questions)} queries for embedding.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:57.985320Z","iopub.execute_input":"2025-05-23T09:07:57.985614Z","iopub.status.idle":"2025-05-23T09:07:57.995594Z","shell.execute_reply.started":"2025-05-23T09:07:57.985589Z","shell.execute_reply":"2025-05-23T09:07:57.994606Z"}},"outputs":[{"name":"stdout","text":"Prepared 3633 documents for embedding.\nPrepared 323 queries for embedding.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(\"Embedding documents... This may take a few minutes.\")\n\n# Embed all documents in the corpus\n# \"RETRIEVAL_DOCUMENT\" is used for items to be retrieved in a search system\ndoc_embeddings = embed_text(docs, model, \"RETRIEVAL_DOCUMENT\", batch_size=25) # Increased batch size for potentially faster processing\nprint(f\"Document embeddings generated. Shape: {doc_embeddings.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:07:57.996574Z","iopub.execute_input":"2025-05-23T09:07:57.996897Z","iopub.status.idle":"2025-05-23T09:12:30.787726Z","shell.execute_reply.started":"2025-05-23T09:07:57.996868Z","shell.execute_reply":"2025-05-23T09:12:30.786936Z"}},"outputs":[{"name":"stdout","text":"Embedding documents... This may take a few minutes.\nDocument embeddings generated. Shape: (3633, 768)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Create a FAISS index\n# IndexFlatL2 performs an exact search using L2 distance (Euclidean distance)\nindex = faiss.IndexFlatIP(doc_embeddings.shape[1])\n\n# Add the document embeddings to the FAISS index\nindex.add(doc_embeddings)\nprint(f\"FAISS index created and {index.ntotal} document embeddings added.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:12:30.788652Z","iopub.execute_input":"2025-05-23T09:12:30.788901Z","iopub.status.idle":"2025-05-23T09:12:30.812510Z","shell.execute_reply.started":"2025-05-23T09:12:30.788880Z","shell.execute_reply":"2025-05-23T09:12:30.811687Z"}},"outputs":[{"name":"stdout","text":"FAISS index created and 3633 document embeddings added.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Example search to test the retrieval system","metadata":{}},{"cell_type":"code","source":"example_query = 'Is Caffeinated Tea Really Dehydrating?'\nprint(f\"Example query: '{example_query}'\")\n\n# Embed the example query\n# \"RETRIEVAL_QUERY\" is used for the search query itself\nexample_query_embedding = embed_text([example_query], model, 'RETRIEVAL_QUERY')\nprint(f\"Example query embedding shape: {example_query_embedding.shape}\")\n\n# Search the FAISS index for the top 3 most similar document\n# k=1 means we want the single most similar document\nk_results = 3\ndistances, retrieved_indices = index.search(example_query_embedding, k_results) #Index search returns a tuple containing two numpy array, distanes and indices.\n\n#retrieved_indices[0] --> This gives the array that represents the indices of the top 3 most similar vector.\n# Using the above indices, we can retrieve the original text chunk\n\nprint(f\"\\nTop {k_results} result(s) for the example query:\")\nfor i in range(k_results):\n    doc_index = retrieved_indices[0][i]\n    score = distances[0][i]\n    retrieved_doc_id = doc_ids[doc_index]\n    retrieved_doc_text = docs[doc_index]\n    print(f\"  Rank {i+1}:\")\n    print(f\"    Score (L2 Distance): {score:.2f}\")\n    print(f\"    Document ID: {retrieved_doc_id}\")\n    print(f\"    Text: \\\"{retrieved_doc_text[:250]}...\\\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:12:30.813468Z","iopub.execute_input":"2025-05-23T09:12:30.813869Z","iopub.status.idle":"2025-05-23T09:12:31.411582Z","shell.execute_reply.started":"2025-05-23T09:12:30.813839Z","shell.execute_reply":"2025-05-23T09:12:31.410724Z"}},"outputs":[{"name":"stdout","text":"Example query: 'Is Caffeinated Tea Really Dehydrating?'\nExample query embedding shape: (1, 768)\n\nTop 3 result(s) for the example query:\n  Rank 1:\n    Score (L2 Distance): 0.75\n    Document ID: MED-4331\n    Text: \"There is a belief that caffeinated drinks, such as tea, may adversely affect hydration. This was investigated in a randomised controlled trial. Healthy resting males (n 21) were recruited from the general population. Following 24 h of abstention from...\"\n  Rank 2:\n    Score (L2 Distance): 0.61\n    Document ID: MED-1853\n    Text: \"PURPOSE: To measure the pH, titratable acidity, fluoride concentration and erosive potential of brewed teas. METHODS: Bag teas were purchased to represent black, green, citrus, fruity, and floral tea flavors from Tulsi, Bigelow, HyVee, Tazo, and Yogi...\"\n  Rank 3:\n    Score (L2 Distance): 0.61\n    Document ID: MED-1645\n    Text: \"BACKGROUND: Tea consumption is associated with decreased cardiovascular risk. Flow-mediated dilatation (FMD) of the brachial artery is related to coronary endothelial function and it is an independent predictor of cardiovascular risk. Black tea has a...\"\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Embed Queries from NFCorpus and Perform Search","metadata":{}},{"cell_type":"code","source":"print(\"Embedding all queries for evaluation... This may take a few minutes.\")\n\n# Embed all queries from the dataset\nquery_embeddings = embed_text(questions, model, \"RETRIEVAL_QUERY\", batch_size=25)\nprint(f\"Query embeddings generated. Shape: {query_embeddings.shape}\")\n\n# Perform search for all queries to get top 10 documents\nk_eval = 10\nprint(f\"Searching top {k_eval} documents for all {len(questions)} queries...\")\n# q_scores contains the distances, q_doc_indices contains the indices in our 'docs' list\nall_query_distances, all_query_retrieved_indices = index.search(query_embeddings, k_eval)\nprint(\"Search complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:12:31.412557Z","iopub.execute_input":"2025-05-23T09:12:31.412865Z","iopub.status.idle":"2025-05-23T09:12:53.913694Z","shell.execute_reply.started":"2025-05-23T09:12:31.412841Z","shell.execute_reply":"2025-05-23T09:12:53.912752Z"}},"outputs":[{"name":"stdout","text":"Embedding all queries for evaluation... This may take a few minutes.\nQuery embeddings generated. Shape: (323, 768)\nSearching top 10 documents for all 323 queries...\nSearch complete.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Evaluation using PyTrec.","metadata":{}},{"cell_type":"markdown","source":"The `pytrec_eval` library requires the search results in a specific dictionary format. We'll convert our FAISS search results into this format. FAISS retuns L2 distance, with metric being lower is better but `pytrec_eval` expects similarity score, with metric being higher is better like nDCG. We will multiply the distances with -1 to get the correct scales.","metadata":{}},{"cell_type":"code","source":"# Create a dictionary of query_id to {doc_id: score} for pytrec_eval\n# Multiply scores (distances) by -1 because smaller L2 distance is better,\n# but pytrec_eval typically expects higher scores for more relevant items for nDCG.\n\nsearch_results_for_eval = {}\nfor i in range(len(q_ids)):\n    query_id = q_ids[i]\n    doc_scores_for_query = {}\n    for j in range(k_eval):\n        retrieved_doc_index = all_query_retrieved_indices[i][j]\n        retrieved_doc_id = doc_ids[retrieved_doc_index]\n        score = all_query_distances[i][j]\n        doc_scores_for_query[retrieved_doc_id] = -1.0 * score.item() # .item() to get Python float\n    search_results_for_eval[query_id] = doc_scores_for_query\n\nprint(f\"Prepared search results for {len(search_results_for_eval)} queries for evaluation.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:14:51.515572Z","iopub.execute_input":"2025-05-23T09:14:51.516023Z","iopub.status.idle":"2025-05-23T09:14:51.528477Z","shell.execute_reply.started":"2025-05-23T09:14:51.515974Z","shell.execute_reply":"2025-05-23T09:14:51.527696Z"}},"outputs":[{"name":"stdout","text":"Prepared search results for 323 queries for evaluation.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"#Example of how one entry looks:\nsample_iterator = iter(search_results_for_eval.items())\n                       \nprint(next(sample_iterator))\nprint(next(sample_iterator))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:23:54.547958Z","iopub.execute_input":"2025-05-23T09:23:54.548791Z","iopub.status.idle":"2025-05-23T09:23:54.553660Z","shell.execute_reply.started":"2025-05-23T09:23:54.548760Z","shell.execute_reply":"2025-05-23T09:23:54.552723Z"}},"outputs":[{"name":"stdout","text":"('PLAIN-2', {'MED-2431': -0.7575794458389282, 'MED-2429': -0.7526673674583435, 'MED-10': -0.7314770221710205, 'MED-14': -0.7240618467330933, 'MED-2439': -0.6640636324882507, 'MED-2428': -0.6509400606155396, 'MED-2434': -0.645990252494812, 'MED-2440': -0.6290814876556396, 'MED-2122': -0.6185914278030396, 'MED-1193': -0.6181725859642029})\n('PLAIN-12', {'MED-1437': -0.6370148062705994, 'MED-2328': -0.6349150538444519, 'MED-2517': -0.6167786717414856, 'MED-2514': -0.6159990429878235, 'MED-1924': -0.6049619913101196, 'MED-4699': -0.6006181836128235, 'MED-2512': -0.5984870791435242, 'MED-3283': -0.5974538326263428, 'MED-4711': -0.5961483120918274, 'MED-2498': -0.5959528684616089})\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Metrics to evaluate\nmetrics_to_evaluate = {'ndcg_cut.10', 'P.1', 'recall.10'} # P_1 is P.1, recall_10 is recall.10\n\n# Evalator\nevaluator = pytrec_eval.RelevanceEvaluator(qrels, metrics_to_evaluate) #Initialize with qrels and metrics you want n\neval_results = evaluator.evaluate(search_results_for_eval) #Evaluate searches\n\n# The result is a dictionary where keys are query_ids and values are dicts of metric scores.\n# We can convert this to a Pandas DataFrame for easier analysis.\ndf_eval_results = pd.DataFrame.from_dict(eval_results, orient='index')\n\nprint(\"Evaluation results per query (sample):\")\nprint(df_eval_results.head())\n\n# Calculate the mean of each metric across all queries\nmean_eval_scores = df_eval_results.mean()\nprint(\"\\nMean evaluation scores across all queries:\")\nprint(mean_eval_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T09:25:08.975746Z","iopub.execute_input":"2025-05-23T09:25:08.976508Z","iopub.status.idle":"2025-05-23T09:25:08.995298Z","shell.execute_reply.started":"2025-05-23T09:25:08.976477Z","shell.execute_reply":"2025-05-23T09:25:08.994350Z"}},"outputs":[{"name":"stdout","text":"Evaluation results per query (sample):\n          P_1  recall_10  ndcg_cut_10\nPLAIN-2   0.0   0.333333     0.507424\nPLAIN-12  0.0   0.066667     0.117858\nPLAIN-23  0.0   0.044444     0.306076\nPLAIN-33  0.0   0.125000     0.285770\nPLAIN-44  1.0   0.086207     0.496100\n\nMean evaluation scores across all queries:\nP_1            0.188854\nrecall_10      0.203507\nndcg_cut_10    0.287975\ndtype: float64\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}