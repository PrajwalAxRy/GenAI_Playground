{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-22T20:41:57.474206Z",
     "iopub.status.busy": "2025-05-22T20:41:57.473864Z",
     "iopub.status.idle": "2025-05-22T20:41:57.851375Z",
     "shell.execute_reply": "2025-05-22T20:41:57.850525Z",
     "shell.execute_reply.started": "2025-05-22T20:41:57.474180Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/vertex-service-sa/peppy-appliance-460613-k3-cc10f0749a25.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search with Vertex AI Embeddings and FAISS on NFCorpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we will build a simple semantic search using Vertex AI's text embedding models and FAISS for semantic search.\n",
    "\n",
    "We are using NFCCourpus Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Steps:**\n",
    "- Setup the libraries\n",
    "- Load the NFCourpus Dataset\n",
    "- Vertex AI to laoad a pre trained model for text embedding.\n",
    "- Convert documents into embeddings and build a FAISS index for fast searching.\n",
    "- Sample Test\n",
    "- Evaluation using pytrec_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:42:11.334371Z",
     "iopub.status.busy": "2025-05-22T20:42:11.333978Z",
     "iopub.status.idle": "2025-05-22T20:43:50.139556Z",
     "shell.execute_reply": "2025-05-22T20:43:50.138161Z",
     "shell.execute_reply.started": "2025-05-22T20:42:11.334349Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.0/288.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for pytrec_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet beir faiss-cpu google-cloud-aiplatform numpy pandas pytrec_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:43:50.142347Z",
     "iopub.status.busy": "2025-05-22T20:43:50.141937Z",
     "iopub.status.idle": "2025-05-22T20:44:16.110101Z",
     "shell.execute_reply": "2025-05-22T20:44:16.109201Z",
     "shell.execute_reply.started": "2025-05-22T20:43:50.142300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/beir/util.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "#beir -> benchmarking Information retrieval. Benchmark framework designed to evaluate the performance of IR models across diverse tasks and dataset.\n",
    "#pytech_eval is python interface to trev_eval toolkit, which is a standard tool for evaluating IR systems.\n",
    "\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "import faiss\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytrec_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:44:16.111406Z",
     "iopub.status.busy": "2025-05-22T20:44:16.110903Z",
     "iopub.status.idle": "2025-05-22T20:44:16.118433Z",
     "shell.execute_reply": "2025-05-22T20:44:16.117510Z",
     "shell.execute_reply.started": "2025-05-22T20:44:16.111382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#This function will take textual strings and convert them into their numerical embeddings using Vertex AI model.\n",
    "#We will be able to use it anywhere for embedding large corpus of data or incoming queries.\n",
    "\n",
    "def embed_text(texts: list[str], model: TextEmbeddingModel, task: str, batch_size: int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embeds a list of texts using a Vertex AI embedding model.\n",
    "\n",
    "    Args:\n",
    "        texts: A list of strings to embed.\n",
    "        model: The Vertex AI TextEmbeddingModel instance.\n",
    "        task: The task type for the embedding (e.g., \"RETRIEVAL_DOCUMENT\", \"RETRIEVAL_QUERY\").\n",
    "        batch_size: The number of texts to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "        A NumPy array containing the embeddings.\n",
    "    \"\"\"\n",
    "    embed_mat = np.zeros((len(texts), 768))  # Assuming 768 dimensions for \"text-embedding-005\"\n",
    "    for batch_start in range(0, len(texts), batch_size):\n",
    "        size = min(len(texts) - batch_start, batch_size)\n",
    "\n",
    "        #Vertex AI SDK method doesn't take a list of raw strings, it expects TextEmbeddingInput objects.\n",
    "        #Each of these objects needs the text and a task_type.\n",
    "        inputs = [TextEmbeddingInput(texts[batch_start + i], task_type=task) for i in range(size)]\n",
    "        embeddings = model.get_embeddings(inputs)\n",
    "        for i in range(size):\n",
    "            embed_mat[batch_start + i, :] = embeddings[i].values\n",
    "    return embed_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading NFCorpus dataset.\n",
    "It contains corpus of medical documents, a set of queries, and relevance judgments (qrels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:44:16.120815Z",
     "iopub.status.busy": "2025-05-22T20:44:16.120463Z",
     "iopub.status.idle": "2025-05-22T20:44:16.147617Z",
     "shell.execute_reply": "2025-05-22T20:44:16.146705Z",
     "shell.execute_reply.started": "2025-05-22T20:44:16.120787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/nfcorpus.zip\"\n",
    "data_path_root = \"datasets\" # Root directory for datasets\n",
    "data_path_nfcorpus = os.path.join(data_path_root, \"nfcorpus\") # Specific path for nfcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:44:16.148805Z",
     "iopub.status.busy": "2025-05-22T20:44:16.148544Z",
     "iopub.status.idle": "2025-05-22T20:44:16.386537Z",
     "shell.execute_reply": "2025-05-22T20:44:16.385841Z",
     "shell.execute_reply.started": "2025-05-22T20:44:16.148784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df72da53599c4f649a69fe9095dbcd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "datasets/nfcorpus.zip:   0%|          | 0.00/2.34M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and unzipped to: datasets/nfcorpus\n"
     ]
    }
   ],
   "source": [
    "# Create the datasets directory if it doesn't exist\n",
    "os.makedirs(data_path_root, exist_ok=True)\n",
    "\n",
    "downloaded_data_path = util.download_and_unzip(url, data_path_root)\n",
    "print(f\"Dataset downloaded and unzipped to: {downloaded_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:44:16.387765Z",
     "iopub.status.busy": "2025-05-22T20:44:16.387486Z",
     "iopub.status.idle": "2025-05-22T20:44:16.479641Z",
     "shell.execute_reply": "2025-05-22T20:44:16.478917Z",
     "shell.execute_reply.started": "2025-05-22T20:44:16.387743Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8b7d8db8d74a899ba49a26a161adba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3633\n",
      "Number of queries: 323\n",
      "Number of query-relevance pairs: 12334\n"
     ]
    }
   ],
   "source": [
    "# Load the corpus, queries, and qrels for the \"test\" split\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=downloaded_data_path).load(split=\"test\")\n",
    "\n",
    "print(f\"Number of documents: {len(corpus)}\")\n",
    "print(f\"Number of queries: {len(queries)}\")\n",
    "print(f\"Number of query-relevance pairs: {sum(len(v) for v in qrels.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:44:16.481400Z",
     "iopub.status.busy": "2025-05-22T20:44:16.481028Z",
     "iopub.status.idle": "2025-05-22T20:44:17.154550Z",
     "shell.execute_reply": "2025-05-22T20:44:17.153515Z",
     "shell.execute_reply.started": "2025-05-22T20:44:16.481365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Document (ID: MED-10): 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland Recent studies have suggested that statins, an established drug group in the prevention of cardiovas...'\n",
      "Example Query (ID: PLAIN-2): 'Do Cholesterol Statin Drugs Cause Breast Cancer?'\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "doc_id_example, doc_example = next(iter(corpus.items()))\n",
    "query_id_example, query_example = next(iter(queries.items()))\n",
    "print(f\"\\nExample Document (ID: {doc_id_example}): '{doc_example['title']} {doc_example['text'][:100]}...'\")\n",
    "print(f\"Example Query (ID: {query_id_example}): '{query_example}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:44:17.155892Z",
     "iopub.status.busy": "2025-05-22T20:44:17.155548Z",
     "iopub.status.idle": "2025-05-22T20:44:43.931593Z",
     "shell.execute_reply": "2025-05-22T20:44:43.930399Z",
     "shell.execute_reply.started": "2025-05-22T20:44:17.155856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.3/196.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade vertexai google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:44:43.933266Z",
     "iopub.status.busy": "2025-05-22T20:44:43.932901Z",
     "iopub.status.idle": "2025-05-22T20:44:43.938102Z",
     "shell.execute_reply": "2025-05-22T20:44:43.937368Z",
     "shell.execute_reply.started": "2025-05-22T20:44:43.933225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------ This is to be hidden ------------------ #\n",
    "#PROJECT_ID = \"Added as kaggle Secret\"\n",
    "#LOCATION = \"Added as Kaggle Secret\"   \n",
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:44:43.941242Z",
     "iopub.status.busy": "2025-05-22T20:44:43.940900Z",
     "iopub.status.idle": "2025-05-22T20:44:43.957283Z",
     "shell.execute_reply": "2025-05-22T20:44:43.956225Z",
     "shell.execute_reply.started": "2025-05-22T20:44:43.941212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"Google_Service_key\", exist_ok=True)\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/kaggle/input/vertex-service-sa/peppy-appliance-460613-k3-cc10f0749a25.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:46:26.119092Z",
     "iopub.status.busy": "2025-05-22T20:46:26.118037Z",
     "iopub.status.idle": "2025-05-22T20:46:26.503067Z",
     "shell.execute_reply": "2025-05-22T20:46:26.502121Z",
     "shell.execute_reply.started": "2025-05-22T20:46:26.119048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "PROJECT_ID = UserSecretsClient().get_secret(\"PROJECT_ID\")\n",
    "LOCATION = UserSecretsClient().get_secret(\"LOCATION\")\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:47:23.414673Z",
     "iopub.status.busy": "2025-05-22T20:47:23.413585Z",
     "iopub.status.idle": "2025-05-22T20:47:25.453467Z",
     "shell.execute_reply": "2025-05-22T20:47:25.452431Z",
     "shell.execute_reply.started": "2025-05-22T20:47:23.414616Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Vertex AI embedding model: text-embedding-005\n"
     ]
    }
   ],
   "source": [
    "#Loading the model for embedding\n",
    "model_name = \"text-embedding-005\" #It has 768 dimensions\n",
    "model = TextEmbeddingModel.from_pretrained(model_name)\n",
    "print(f\"Loaded Vertex AI embedding model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:47:27.224091Z",
     "iopub.status.busy": "2025-05-22T20:47:27.223737Z",
     "iopub.status.idle": "2025-05-22T20:47:27.239753Z",
     "shell.execute_reply": "2025-05-22T20:47:27.238889Z",
     "shell.execute_reply.started": "2025-05-22T20:47:27.224066Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 3633 documents for embedding.\n",
      "Prepared 323 queries for embedding.\n"
     ]
    }
   ],
   "source": [
    "#corpus.items()\n",
    "\n",
    "#zip and '*', the unpacking operator takes the list of tuples and \"unzips\" them into two separate tuples.\n",
    "doc_ids, docs = zip(*[(doc_id, doc['text']) for doc_id, doc in corpus.items()])\n",
    "print(f\"Prepared {len(docs)} documents for embedding.\")\n",
    "\n",
    "# Extract query IDs and texts\n",
    "q_ids, questions = zip(*[(q_id, q_text) for q_id, q_text in queries.items()])\n",
    "print(f\"Prepared {len(questions)} queries for embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:47:27.600716Z",
     "iopub.status.busy": "2025-05-22T20:47:27.600384Z",
     "iopub.status.idle": "2025-05-22T20:52:22.796295Z",
     "shell.execute_reply": "2025-05-22T20:52:22.795448Z",
     "shell.execute_reply.started": "2025-05-22T20:47:27.600691Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding documents... This may take a few minutes.\n",
      "Document embeddings generated. Shape: (3633, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding documents... This may take a few minutes.\")\n",
    "\n",
    "# Embed all documents in the corpus\n",
    "# \"RETRIEVAL_DOCUMENT\" is used for items to be retrieved in a search system\n",
    "doc_embeddings = embed_text(docs, model, \"RETRIEVAL_DOCUMENT\", batch_size=25) # Increased batch size for potentially faster processing\n",
    "print(f\"Document embeddings generated. Shape: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:52:22.798005Z",
     "iopub.status.busy": "2025-05-22T20:52:22.797720Z",
     "iopub.status.idle": "2025-05-22T20:52:22.825282Z",
     "shell.execute_reply": "2025-05-22T20:52:22.824391Z",
     "shell.execute_reply.started": "2025-05-22T20:52:22.797982Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and 3633 document embeddings added.\n"
     ]
    }
   ],
   "source": [
    "# Create a FAISS index\n",
    "# IndexFlatL2 performs an exact search using L2 distance (Euclidean distance)\n",
    "index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
    "\n",
    "# Add the document embeddings to the FAISS index\n",
    "index.add(doc_embeddings)\n",
    "print(f\"FAISS index created and {index.ntotal} document embeddings added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example search to test the retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:52:22.826572Z",
     "iopub.status.busy": "2025-05-22T20:52:22.826342Z",
     "iopub.status.idle": "2025-05-22T20:52:23.487800Z",
     "shell.execute_reply": "2025-05-22T20:52:23.486837Z",
     "shell.execute_reply.started": "2025-05-22T20:52:22.826554Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example query: 'Is Caffeinated Tea Really Dehydrating?'\n",
      "Example query embedding shape: (1, 768)\n",
      "\n",
      "Top 3 result(s) for the example query:\n",
      "  Rank 1:\n",
      "    Score (L2 Distance): 0.75\n",
      "    Document ID: MED-4331\n",
      "    Text: \"There is a belief that caffeinated drinks, such as tea, may adversely affect hydration. This was investigated in a randomised controlled trial. Healthy resting males (n 21) were recruited from the general population. Following 24 h of abstention from...\"\n",
      "  Rank 2:\n",
      "    Score (L2 Distance): 0.61\n",
      "    Document ID: MED-1853\n",
      "    Text: \"PURPOSE: To measure the pH, titratable acidity, fluoride concentration and erosive potential of brewed teas. METHODS: Bag teas were purchased to represent black, green, citrus, fruity, and floral tea flavors from Tulsi, Bigelow, HyVee, Tazo, and Yogi...\"\n",
      "  Rank 3:\n",
      "    Score (L2 Distance): 0.61\n",
      "    Document ID: MED-1645\n",
      "    Text: \"BACKGROUND: Tea consumption is associated with decreased cardiovascular risk. Flow-mediated dilatation (FMD) of the brachial artery is related to coronary endothelial function and it is an independent predictor of cardiovascular risk. Black tea has a...\"\n"
     ]
    }
   ],
   "source": [
    "example_query = 'Is Caffeinated Tea Really Dehydrating?'\n",
    "print(f\"Example query: '{example_query}'\")\n",
    "\n",
    "# Embed the example query\n",
    "# \"RETRIEVAL_QUERY\" is used for the search query itself\n",
    "example_query_embedding = embed_text([example_query], model, 'RETRIEVAL_QUERY')\n",
    "print(f\"Example query embedding shape: {example_query_embedding.shape}\")\n",
    "\n",
    "# Search the FAISS index for the top 3 most similar document\n",
    "# k=1 means we want the single most similar document\n",
    "k_results = 3\n",
    "distances, retrieved_indices = index.search(example_query_embedding, k_results) #Index search returns a tuple containing two numpy array, distanes and indices.\n",
    "\n",
    "#retrieved_indices[0] --> This gives the array that represents the indices of the top 3 most similar vector.\n",
    "# Using the above indices, we can retrieve the original text chunk\n",
    "\n",
    "print(f\"\\nTop {k_results} result(s) for the example query:\")\n",
    "for i in range(k_results):\n",
    "    doc_index = retrieved_indices[0][i]\n",
    "    score = distances[0][i]\n",
    "    retrieved_doc_id = doc_ids[doc_index]\n",
    "    retrieved_doc_text = docs[doc_index]\n",
    "    print(f\"  Rank {i+1}:\")\n",
    "    print(f\"    Score (L2 Distance): {score:.2f}\")\n",
    "    print(f\"    Document ID: {retrieved_doc_id}\")\n",
    "    print(f\"    Text: \\\"{retrieved_doc_text[:250]}...\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7489704,
     "sourceId": 11913321,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
